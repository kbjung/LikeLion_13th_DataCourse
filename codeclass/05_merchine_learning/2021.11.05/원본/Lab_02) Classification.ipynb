{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iItkhsGB5c4W"
   },
   "source": [
    "# Lab 2\n",
    "### Context\n",
    "\n",
    "#### Classification\n",
    "+ Logistic Regression\n",
    "+ Support Vector Machine\n",
    "+ Decision Tree\n",
    "+ Random Forest\n",
    "+ XGBoost\n",
    "+ LightGBM\n",
    "\n",
    "#### Evaluation\n",
    "+ Accuracy\n",
    "+ Confusion Matrix\n",
    "+ ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAfXTYpj5ysB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLQ6DEV7GN0S"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "\n",
    "train_path = join(BASE_DIR, 'data', 'MDC14', 'train.csv')\n",
    "test_path  = join(BASE_DIR, 'data', 'MDC14', 'test.csv')\n",
    "\n",
    "data = pd.read_csv(train_path)\n",
    "x_test = pd.read_csv(test_path)\n",
    "\n",
    "label = data['credit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HniR48pCGN0S"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5VRQ3DhGN0T"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lKqkhM7GN0T"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wamwiXSxGN0U"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPh9ImpqGN0U"
   },
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHEskNSEGN0V"
   },
   "outputs": [],
   "source": [
    "x_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QERSh0OdGN0V"
   },
   "outputs": [],
   "source": [
    "x_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZfkXZa4GN0W"
   },
   "outputs": [],
   "source": [
    "# 불필요한 컬럼 제거\n",
    "data.drop(columns=['index', 'credit'], inplace=True)\n",
    "x_test.drop(columns=['index'],         inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aWu-NXuGN0W"
   },
   "outputs": [],
   "source": [
    "cat_columns = [c for c, t in zip(data.dtypes.index, data.dtypes) if t == 'O'] \n",
    "num_columns = [c for c    in data.columns if c not in cat_columns]\n",
    "\n",
    "print('Categorical Columns: \\n{}\\n'.format(cat_columns))\n",
    "print('Numeric Columns: \\n{}'.format(num_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvu179EHGN0W"
   },
   "outputs": [],
   "source": [
    "label.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YetDFNU_GN0X"
   },
   "source": [
    "### 실습 1 (15분)\n",
    "- 이번 주차부터는 Train 데이터셋을 Train, Valid로 쪼개서 모델 검증에 사용할 데이터를 추가할 예정입니다.\n",
    "    - 이전에도 말씀드렸듯 쪼갠 Train 데이터에 Valid 또는 Test 데이터의 정보가 들어가지 않도록 주의해주세요.\n",
    "    - 데이터를 쪼갤때는 Scikit-learn의 model_selection 패키지에 있는 train_test_split() 함수를 사용합니다.\n",
    "\n",
    "1. 결측치 확인 및 결측치 처리\n",
    "2. 스케일링\n",
    "3. 범주형 변수 OneHot Encoding, 라벨 변수 Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldLSr6n_GN0X"
   },
   "source": [
    "#### 1. 결측치 확인 및 결측치 처리 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndX7qBtbGN0Y"
   },
   "outputs": [],
   "source": [
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOGBk3QWGN0Y"
   },
   "outputs": [],
   "source": [
    "msno.matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I2ODj6AGN0Y"
   },
   "outputs": [],
   "source": [
    "msno.matrix(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqWyaTeXGN0Y"
   },
   "source": [
    "##### 수치형 변수들의 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d19ke7fKGN0Z"
   },
   "outputs": [],
   "source": [
    "pd.isna(data[num_columns]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWU_b0akGN0Z"
   },
   "outputs": [],
   "source": [
    "pd.isna(x_test[num_columns]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DxWGhOVGN0Z"
   },
   "source": [
    "##### 범주형 변수들의 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQXtNBj7GN0a"
   },
   "outputs": [],
   "source": [
    "pd.isna(data[cat_columns]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jF9WSU9GN0a"
   },
   "outputs": [],
   "source": [
    "pd.isna(x_test[cat_columns]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnMiUz0JGN0a"
   },
   "source": [
    "##### 데이터 쪼개기, Train -> (Train, Valid)\n",
    "- train_test_split 파라미터 \n",
    "    - test_size  (float): Valid(test)의 크기의 비율을 지정\n",
    "    - random_state (int): 데이터를 쪼갤 때 내부적으로 사용되는 난수 값 (해당 값을 지정하지 않으면 매번 달라집니다.)\n",
    "    - shuffle     (bool): 데이터를 쪼갤 때 섞을지 유무\n",
    "    - stratify   (array): Stratify란, 쪼개기 이전의 클래스 비율을 쪼개고 나서도 유지하기 위해 설정해야하는 값입니다. 클래스 라벨을 넣어주면 됩니다.\n",
    "        - ex) 원본 Train 데이터의 클래스 비율이 (7:3) 이었다면, 쪼개어진 Train, Valid(test) 데이터의 클래스 비율도 (7:3)이 됩니다. 당연히 분류 데이터에서만 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amkAg8PpGN0a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXuMPHhDGN0b"
   },
   "outputs": [],
   "source": [
    "# 쪼개어진 Train, Valid 데이터의 비율은 (7:3), 내부 난수 값 42, 데이터를 쪼갤 때 섞으며 label 값으로 Stratify 하는 코드 입니다. random_state를 주석 처리하고 데이터를 확인해보시면 계속 바뀝니다.\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(data, label, \n",
    "                                                      test_size=0.3,\n",
    "                                                      random_state=42,\n",
    "                                                      shuffle=True,\n",
    "                                                      stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bV8HKv_OGN0b"
   },
   "outputs": [],
   "source": [
    "# 쪼갠 데이터의 인덱스는 정리해주는것이 좋습니다. pd.concat 연산 시, 인덱스를 기준으로 연결하기 때문입니다.\n",
    "# drop 인자를 True로 주지 않으면 이전 인덱스가 새로운 변수로 생성됩니다.\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "x_valid = x_valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32zvpd3CGN0b"
   },
   "source": [
    "### 1) 전처리 실습 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhCfmTKIGN0b"
   },
   "source": [
    "#### 1. 결측치 처리\n",
    "`occyp_type` 변수 최빈 값으로 결측치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBlXHxwhGN0c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdd2yloqGN0c"
   },
   "source": [
    "#### 2. 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9ak8RaBGN0c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jPCVjUEGN0c"
   },
   "source": [
    "#### 3. 범주형 변수 OneHot Encoding, 라벨 변수 Label Encoding \n",
    "- 범주형 변수 OneHot Encoding 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZEHFWr8GN0c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T54m72IeGN0d"
   },
   "source": [
    "## Classification\n",
    "머신러닝과 통계학에서의 분류는 새로 관측된 데이터가 어떤 범주 집합에 속하는지를 식별하는 것을 말합니다. <br>\n",
    "훈련 데이터를 이용해 모델을 학습하면, 모델은 결정 경계(Decision boundary)라는 데이터를 분류하는 선을 만들어 냅니다.<br>\n",
    "이번 수업에서는 여러가지 대표적인 모델의 원리를 간단히 알아보고, 결정 경계를 만들어 데이터를 분류해보겠습니다.<br>\n",
    "\n",
    "<img src='./img/Classification.png' height=600 width=600 align='left'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAJvUmobGN0d"
   },
   "source": [
    "### Logistic Regression\n",
    "Regression 이라는 말에서 알 수 있듯이, 로지스틱 회귀 모델은 선형 회귀 모델에서 변형된 모델입니다. <br>\n",
    "Odds라는 어떤 일이 발생할 상대적인 비율 개념을 사용해 선형 회귀식을 변형합니다.\n",
    "\n",
    "$$ Odds = {p \\over {1-p}} $$\n",
    "$$ p : 어떤\\ 일이\\ 발생할\\ 확률 $$\n",
    "\n",
    "Odds를 그대로 사용하지말고 log를 취해 사용하면 0을 기준으로 상호 대칭적이며, 계산을 수월하게 할 수 있도록 변경해줍니다.<br>\n",
    "기존의 선형 회귀식에서 y 위치에 log Odds를 적용하면 다음과 같은 식이 됩니다.<br>\n",
    "\n",
    "$$ ln({Y \\over {1-Y}}) = wx + b $$\n",
    "\n",
    "<img src='./img/logodd.png' height=600 width=600/>\n",
    "\n",
    "이를 y에 대해 정리하면 그 유명한 sigmoid 식이 됩니다.\n",
    "\n",
    "$$ y = {1 \\over {1+\\exp^{-(wx + b)}}} $$\n",
    "\n",
    "\n",
    "Logistic Regression은 Maximum Likelihood Estimation(MLE)이라는 과정을 통해 모델을 학습하는데, 자세한 내용은 참조 목록에 있는 페이지를 확인해주시면 감사하겠습니다.<br>\n",
    "\n",
    "### 로지스틱 회귀은 이진 분류 모델로 알고 있는데, 어떻게 여러개의 클래스를 분류할 수 있나요?\n",
    "하나의 수식이 출력하는 결과는 클래스의 확률을 나타내는 것은 맞습니다. 하지만, 멀티 클래스인 경우 내부적으로 클래스 수에 맞게 여러개의 수식을 만들어 각각의 클래스에 속할 확률을 계산한 후 가장 높은 확률은 가진 클래스로 분류합니다. 이를 One-vs-Rest라고 합니다. 자세한 내용은 참조 목록에 있는 페이지를 확인해주시면 감사하겠습니다.\n",
    "\n",
    "Logistic Regression은 Sklearn의 linear_model 패키지에 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w86sYhT2GN0d"
   },
   "source": [
    "- Logistic Regression 대표적 파라미터\n",
    "    - C             (float): 얼마나 모델에 규제를 넣을지 결정하는 값 작아질수록 모델에 규제가 높아짐 (과적합 방지, from SVM) \n",
    "    - fit_intercept (bool) : 회귀 수식에서 y 절편을 포함할지 유무\n",
    "    - random_state  (int)  : 내부적으로 사용되는 난수값\n",
    "    - l1_ratio      (float): 규제 항을 얼마나 많이 적용할지 \n",
    "    - class_weight  (dict) : 학습 시 클래스의 비율에 맞춰 손실값에 가중치를 부여\n",
    "    \n",
    "    \n",
    "#### ref\n",
    "- [Classification, Wiki](https://en.wikipedia.org/wiki/Statistical_classification)\n",
    "- [로지스틱 회귀 파라미터 추정](https://ratsgo.github.io/machine%20learning/2017/07/02/logistic/)\n",
    "- [Scikit-learn, Logistic Regression](https://www.google.com/url?q=http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html&sa=U&ved=0ahUKEwibhIa0-uDhAhVPeXAKHfPhCYQQFggEMAA&client=internal-uds-cse&cx=016639176250731907682:tjtqbvtvij0&usg=AOvVaw2AirAop04TUH9X2S1r9FVd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjaKfTShGN0e"
   },
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h9AyTqoGN0f"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiWpeFCRGN0f"
   },
   "source": [
    "#### 2) 모델 학습하기 (훈련 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xroggB3NGN0f"
   },
   "outputs": [],
   "source": [
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ds_yeevGN0f"
   },
   "source": [
    "#### 3) 결과 예측하기 (검증 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdIEryueGN0f"
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(x_valid)\n",
    "y_pred_proba = lr.predict_proba(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIWpFG5_GN0g"
   },
   "source": [
    "#### 4) 결과 살펴보기\n",
    "일반적으로 분류에서는 Accuracy, 정확도를 평가 척도로 사용합니다. + Log loss<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIkRROaVGN0g"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5sxng_YGN0g"
   },
   "outputs": [],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QAhx8o8GN0g"
   },
   "outputs": [],
   "source": [
    "y_pred_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sdCutPRGN0h"
   },
   "outputs": [],
   "source": [
    "print('로지스틱 회귀, 정확도 : {:.4f}'.format(accuracy_score(y_valid, y_pred)))\n",
    "print('로지스틱 회귀, Log loss : {:.4f}'.format(log_loss(y_valid, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcphnPAcGN0h"
   },
   "source": [
    "#### 로지스틱 회귀 모델의 계수 w, 절편 b 살펴보기\n",
    "어떤 변수에 얼마 만큼의 가중치가 할당되고, 절편 값은 얼마나 할당되는지 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z76mwnW-GN0h"
   },
   "outputs": [],
   "source": [
    "print('로지스틱 회귀, 계수(w) : {}, 절편(b) : {}'.format(lr.coef_, lr.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL_N-i7iGN0h"
   },
   "source": [
    "### Support Vector Machine\n",
    "Support Vector Machine(SVM, 서포트 벡터 머신)는 주어진 데이터를 바탕으로하여 두 카테고리(이진 분류의 경우) 사이의 간격(Margin, 마진)을 최대화하는 데이터 포인트(Support Vector, 서포트 벡터)를 찾아내고, <br>\n",
    "그 서포트 벡터에 수직인 경계를 통해 데이터를 분류하는 알고리즘입니다.<br><br>\n",
    "<img src=\"./img/SVM.png\" alt=\"Support Vector machine\" style=\"height: 400px\" align='center'/>\n",
    "\n",
    "### 왜 마진을 최대화 할까요?\n",
    "서포트 벡터 머신에서 나오는 마진은 물건을 판매할때 마진이 20%다 라고 말하는 그 마진이 맞습니다. <br>\n",
    "그렇다면 경계면과의 마진을 최대화 하는 것이 왜 분류를 잘하게 할까요? \n",
    "\n",
    "#### 경험적 위험 최소화(Empirical Risk Minimization, ERM) vs 구조적 위험 최소화(Structural Risk Minimization,SRM)\n",
    "* 경험적 위험 최소화 \n",
    "    * 훈련 데이터에 대해 위험을 최소화\n",
    "    * 학습 알고리즘의 목표\n",
    "    * 뉴럴 네트워크, 결정 트리, 선형 회귀, 로지스틱 회귀 등.\n",
    "* 구조적 위험 최소화\n",
    "    * 관찰하지 않은(Unseen) 데이터에 대해서도 위험을 최소화\n",
    "    * 오차 최소화를 일반화 시키는 것\n",
    "    \n",
    "<div align='center'> \n",
    "    <font size=\"6\">어떤 모델이 더 좋을까요?</font> \n",
    "</div>\n",
    "\n",
    "\n",
    "<img src=\"./img/ERM_SRM.png\" alt=\"ERM_SRM\" style=\"height: 300px\" align='center'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq1-CMsiGN0i"
   },
   "source": [
    "### Cost : Soft or Hard\n",
    "SVM에는 Soft Margin, Hard Margin 이라는 말이 있습니다. 단어 자체에서도 유추할 수 있으시겠지만, Soft Margin은 유연한 경계면을 만들어내고 Hard Margin은 분명하게 나누는 경계면을 만들어냅니다. 그렇다면 왜 Soft Margin이 필요한걸까요?\n",
    "\n",
    "<img src = \"./img/Softmargin.png\" alt=\"Softmargin\" style=\"height: 400px\" />\n",
    "\n",
    "다음과 같은 데이터 분포는 직선으로 두개의 데이터를 나누는 경계면을 만들기 어렵습니다. 현실에서도 우리가 최적의 답을 찾지 못할때 어느정도 비용(Cost, C)을 감수하면서 적절한 답을 찾는 것을 떠올려보세요.<br>\n",
    "Soft Margin은 그런 원리입니다. 경계면을 조금씩 넘어가는 데이터들(비용, Cost)을 감수하면서 가장 차선의 경계면을 찾습니다.<br>\n",
    "실제 알고리즘에서도 C(Cost)값을 통해 얼마나 비용을 감수할 것인지를 결정할 수 있습니다. 크면 클수록 Soft Margin을, 작으면 작을수록 Hard Margin을 만들어냅니다. <br>\n",
    "\n",
    "\n",
    "### 저차원을 고차원으로 Kernel Trick\n",
    "SVM은 기본적으로 선형 분류를 위한 경계면을 만들어냅니다. 그렇다면 어떻게 비선형 분류를 할 수 있을까요?<br>\n",
    "\n",
    "<img src = \"./img/Hyperplane.png\" alt=\"hyper\" style=\"height: 300px\" />\n",
    "\n",
    "저차원(2차원)에서는 선형 분리가 되지 않을 수 있지만, 고차원(3차원)에서는 선형 분리가 가능할 수 있습니다.<br>\n",
    "이러한 원리를 바탕으로 선형 분리가 불가능한 저차원 데이터를 선형 분리가 가능한 어떤 고차원으로 보내 선형 분리를 할 수 있습니다.<br>\n",
    "하지만, 저차원 데이터를 고차원으로 보내서 서포트 벡터를 구하고 저차원으로 내리는 과정에서 더 복잡해지고 연산량도 많아질것이 분명합니다. <br>\n",
    "그래서 여기에서 Kernel Trick이라는 Mapping 함수를 사용합니다. Kernel Trick은 고차원 Mapping과 고차원에서의 내적 연산을 한번에 할 수 있는 방법입니다. <br>\n",
    "이를 통해 여러가지 Kernel 함수를 통해 저차원에서 해결하지 못한 선형 분리를 고차원에서 해결할 수 있습니다.<br>\n",
    "\n",
    "대표적인 Kernel 함수\n",
    "- Linear (선형 함수)\n",
    "- Poly   (다항식 함수)\n",
    "- RBF    (방사기저 함수)\n",
    "- Hyper-Tangent (쌍곡선 탄젠트 함수)\n",
    "\n",
    "서포트 벡터 머신 분류기는 Sklearn의 svm 패키지에 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v-6wfkvGN0i"
   },
   "source": [
    "- SVM 대표적 파라미터\n",
    "    - C             (float): 얼마나 모델에 규제를 넣을지 결정하는 값 작아질수록 모델에 규제가 높아짐 (과적합 방지, from SVM, Hard Margin) \n",
    "    - degree        (int)  : Poly Kernel 사용 시, 차수를 결정하는 값\n",
    "    - kernel        (str)  : Kernel trick에 사용할 커널 종류\n",
    "    - random_state  (int)  : 내부적으로 사용되는 난수값\n",
    "    - class_weight  (dict) : 학습 시 클래스의 비율에 맞춰 손실값에 가중치를 부여\n",
    "    - gamma         (float): 모델이 생성하는 경계가 복잡해지는 정도 (값이 커질수록 데이터 포인터가 영향력을 행사하는 거리가 짧아져 경계가 복잡해진다.)\n",
    "    \n",
    "### ref\n",
    "- [Scikit-learn SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiUWfRUEGN0i"
   },
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwhflbV8GN0j"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(probability=True, max_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s15SGhVNGN0j"
   },
   "source": [
    "#### 2) 모델 학습하기 (훈련 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGdJtzzlGN0j"
   },
   "outputs": [],
   "source": [
    "svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmJoMOQKGN0j"
   },
   "source": [
    "#### 3) 결과 예측하기 (검증 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXMYuRk4GN0j"
   },
   "outputs": [],
   "source": [
    "y_pred = svc.predict(x_valid)\n",
    "y_pred_proba = svc.predict_proba(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfJco11XGN0k"
   },
   "source": [
    "#### 4) 결과 살펴보기\n",
    "일반적으로 분류에서는 Accuracy, 정확도를 평가 척도로 사용합니다. + Log loss<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VcVr3yTGN0k"
   },
   "outputs": [],
   "source": [
    "print('서포트 벡터 머신, 정확도 : {:.4f}'.format(accuracy_score(y_valid, y_pred)))\n",
    "print('서포트 벡터 머신, Log loss : {:.4f}'.format(log_loss(y_valid, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVCJzN8AGN0k"
   },
   "source": [
    "### Decision Tree\n",
    "이전의 회귀 수업에서 결정 트리에 대해 간단하게 살펴보았었습니다.<br>\n",
    "결정 트리는 입력 변수를 특정한 기준으로 잘라(분기) 트리 형태의 구조로 분류를 하는 모델입니다.<br>\n",
    "\n",
    "<img src='./img/DTDesc.png' style=\"height: 400px\"  />\n",
    "<br>\n",
    "\n",
    "* 사람의 논리적 사고 방식을 모사하는 분류 방법론\n",
    "* IF-THEN rule의 조합으로 class 분류\n",
    "* 결과를 나무 모양으로 그릴 수 있음\n",
    "* Greedy 한 알고리즘 (한번 분기하면 이후에 최적의 트리 형태가 발견되더라도 되돌리지 않음, 최적의 트리 생성을 보장하지 않음)\n",
    "* 축에 직교하는 분기점\n",
    "\n",
    "<img src='./img/DT_G.png' style=\"height: 400px\"  />\n",
    "\n",
    "#### 불순도(Impurity, Entropy)\n",
    "결정 트리는 데이터의 불순도를 최소화 할 수 있는 방향으로 트리를 분기합니다. <br>\n",
    "불순도란 정보 이론(Information Theory)에서 말하는 얻을 수 있는 정보량이 많은 정도를 뜻합니다. <br>\n",
    "정보 이론의 정보량의 자세한 내용은 참조 목록에 링크로 남겨두겠습니다. 확인해보시면 좋겠습니다.\n",
    "\n",
    "결정 트리 모델은 Sklearn의 tree 패키지에 있습니다.\n",
    "\n",
    "#### 가지치기 (Pruning)\n",
    "풀트리를 생성하게되면 모델이 과적합되는데, 이를 방지하기 위한 방법으로 가지치기를 진행합니다.<br>\n",
    "트리의 크기를 제한하거나, 리프 노드의 데이터셋 개수를 제한하는 방식으로 가지치기를 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3cMEbNJGN0l"
   },
   "source": [
    "- Decision Tree 대표적 파라미터\n",
    "    - crierion          (str): 정보량 계산 시 사용할 수식 (gini, entropy)\n",
    "    - max_depth         (int): 생성할 결정 트리의 높이\n",
    "    - min_samples_split (int): 분기를 수행하는 최소한의 데이터 수\n",
    "    - max_leaf_nodes    (int): 리프 노드에서 가지고 있을 수 있는 최대 데이터 수\n",
    "    - random_state      (int): 내부적으로 사용되는 난수값\n",
    "    \n",
    "#### ref\n",
    "- [Information Theory, ratsgo님 블로그](https://ratsgo.github.io/statistics/2017/09/22/information/)\n",
    "- [Scikit-learn, Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xAmIdv5GN0l"
   },
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjPKjMx9GN0l"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLRiyLYxGN0m"
   },
   "source": [
    "#### 2) 모델 학습하기 (훈련 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyF0CxFoGN0m"
   },
   "outputs": [],
   "source": [
    "dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTX6oeONGN0m"
   },
   "source": [
    "#### 3) 결과 예측하기 (검증 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bO7w6XQGN0m"
   },
   "outputs": [],
   "source": [
    "y_pred = dt.predict(x_valid)\n",
    "y_pred_proba = dt.predict_proba(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL07Pu20GN0m"
   },
   "source": [
    "#### 4) 결과 살펴보기\n",
    "일반적으로 분류에서는 Accuracy, 정확도를 평가 척도로 사용합니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZSfNqbwGN0n"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "print('결정 트리, 정확도 : {:.4f}'.format(accuracy_score(y_valid, y_pred)))\n",
    "print('결정 트리, Log loss : {:.4f}'.format(log_loss(y_valid, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGaWM-eVGN0n"
   },
   "source": [
    "#### Feature Importance\n",
    "트리 기반 모델은 트리를 분기하는 과정에서 어떤 변수가 모델을 생성하는데 중요한지에 대한 변수 중요도를 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_RzzzK-GN0n"
   },
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(dt.feature_importances_.reshape((1, -1)), columns=x_train.columns, index=['feature_importance'])\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2IjmK8yGN0n"
   },
   "source": [
    "### Random Forest\n",
    "결정 트리가 나무였다면, 랜덤 포레스트는 숲 입니다. 랜덤 포레스트의 특징은 작은 트리들을 여러개 만들어 합치는 모델입니다.<br>\n",
    "서로 다른 변수 셋으로 여러 트리를 생성합니다. 여러개의 모델을 합치는 앙상블 기법 중 대표적인 예시입니다.<br>\n",
    "랜덤 포레스트 모델은 Sklearn의 ensemble 패키지에 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "145KaE0IGN0o"
   },
   "source": [
    "- RandomForest 대표적 파라미터\n",
    "    - n_estimators      (int)  : 내부에서 생성할 결정 트리의 개수 \n",
    "    - crierion          (str)  : 정보량 계산 시 사용할 수식 (gini, entropy)\n",
    "    - max_depth         (int)  : 생성할 트리의 높이\n",
    "    - min_samples_split (int)  : 분기를 수행하는 최소한의 데이터 수\n",
    "    - max_leaf_nodes    (int)  : 리프 노드에서 가지고 있을 수 있는 최대 데이터 수\n",
    "    - random_state      (int)  : 내부적으로 사용되는 난수값\n",
    "    - n_jobs            (int)  : 병렬처리에 사용할 CPU 수\n",
    "    - class_weight      (dict) : 학습 시 클래스의 비율에 맞춰 손실값에 가중치를 부여\n",
    "    \n",
    "#### ref\n",
    "- [Scikit-learn, Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7IHwLXfGN0o"
   },
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgbbWo3fGN0o"
   },
   "outputs": [],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlD9D83yGN0o"
   },
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RF1B9QQbGN0o"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=500, n_jobs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri7xS-q_GN0p"
   },
   "source": [
    "#### 2) 모델 학습하기 (훈련 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKwj0pnLGN0p"
   },
   "outputs": [],
   "source": [
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUBVWh6pGN0p"
   },
   "source": [
    "#### 3) 결과 예측하기 (검증 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSTUWUrXGN0p"
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(x_valid)\n",
    "y_pred_proba = rf.predict_proba(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jAB_tdWGN0p"
   },
   "source": [
    "#### 4) 결과 살펴보기\n",
    "일반적으로 분류에서는 Accuracy, 정확도를 평가 척도로 사용합니다. + Log loss<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4AmgCnGGN0q"
   },
   "outputs": [],
   "source": [
    "print('랜덤 포레스트, 정확도 : {:.4f}'.format(accuracy_score(y_valid, y_pred)))\n",
    "print('랜덤 포레스트, Log loss : {:.4f}'.format(log_loss(y_valid, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jobF7FuvGN0q"
   },
   "source": [
    "#### Feature Importance\n",
    "트리 기반 모델은 트리를 분기하는 과정에서 어떤 변수가 모델을 생성하는데 중요한지에 대한 변수 중요도를 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yh5UhtI3GN0q"
   },
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(rf.feature_importances_.reshape((1, -1)), columns=x_train.columns, index=['feature_importance'])\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByCd2Wv2GN0q"
   },
   "source": [
    "### 같이 해보는 실습 (10분)\n",
    "#### 위에서 사용한 것과 같이 XGBoost, LightGBM 모델을 사용해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tHfplgWGN0r"
   },
   "source": [
    "### XGBoost\n",
    "Random Forest는 전체 데이터 샘플과 변수를 사용하는 것이 아닌, 임의로 데이터 샘플 및 변수를 선택하여 학습하는 부트스트래핑(Bootstrapping) 방식의 앙상블 알고리즘 입니다.<br>\n",
    "XGBoost는 이름에서도 알 수 있듯 각 이터레이션에서 맞추지 못한 샘플에 대해 가중치를 부여하여 모델을 학습시키는 부스팅(Boosting) 계열의 트리 모델입니다.<br>\n",
    "부스팅 알고리즘 자체도 강력하지만, XGBoost는 강력한 병렬 처리 성능과 자동 가지치기(Pruning) 알고리즘이 적용되어 기존 부스팅 알고리즘보다 과적합 방지에 이점이 있습니다.<br>\n",
    "또한 자체 교차검증 알고리즘과 결측치 자체처리 기능을 가지고 있습니다. \n",
    "다른 부스팅 알고리즘과 동일하게 `균형 트리 분할 방식`으로 모델을 학습하여 대칭적인 트리를 형성하게됩니다.\n",
    "\n",
    "1. Gradient Boosting Model 대비 빠른 수행시간\n",
    "2. 과적합 규제 기능(Regularization)\n",
    "3. 가지치기 기능(Tree pruning)\n",
    "4. 자체 내장 교차 검증 기능\n",
    "5. 결측치 자체 처리(결측치 처리를 하지 않아도 모델 사용 가능)\n",
    "6. Early Stopping 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPIS45E8GN0r"
   },
   "source": [
    "- XGBoost 대표적 파라미터\n",
    "    - n_estimators      (int)  : 내부에서 생성할 결정 트리의 개수 \n",
    "    - max_depth         (int)  : 생성할 결정 트리의 높이\n",
    "    - learning_rate     (float): 훈련량, 학습 시 모델을 얼마나 업데이트할지 결정하는 값\n",
    "    - colsample_bytree  (float): 열 샘플링에 사용하는 비율\n",
    "    - subsample         (float): 행 샘플링에 사용하는 비율\n",
    "    - reg_alpha         (float): L1 정규화 계수\n",
    "    - reg_lambda        (float): L2 정규화 계수\n",
    "    - booster           (str)  : 부스팅 방법 (gblinear / gbtree / dart)\n",
    "    - random_state      (int)  : 내부적으로 사용되는 난수값\n",
    "    - n_jobs            (int)  : 병렬처리에 사용할 CPU 수\n",
    "    \n",
    "#### ref\n",
    "- [Gradient Boosting 3months님 블로그](https://3months.tistory.com/368)\n",
    "- [XGBoost rosypark님 블로그](https://rosypark.tistory.com/59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOyDY5PKGN0r"
   },
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgyEmb62GN0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TCjgAU9GN0r"
   },
   "source": [
    "#### 2) 모델 학습하기 (훈련 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHZqqp7IGN0s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHW1RujWGN0s"
   },
   "source": [
    "#### 3) 결과 예측하기 (검증 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NJxeXUeGN0s"
   },
   "outputs": [],
   "source": [
    "y_pred = \n",
    "y_pred_proba =  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-31J-V0GGN0s"
   },
   "source": [
    "#### 4) 결과 살펴보기\n",
    "일반적으로 분류에서는 Accuracy, 정확도를 평가 척도로 사용합니다. + Log loss<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h3-JltTGN0s"
   },
   "outputs": [],
   "source": [
    "print('XGBoost, 정확도 : {:.4f}'.format(accuracy_score(y_valid, y_pred)))\n",
    "print('XGBoost, Log loss : {:.4f}'.format(log_loss(y_valid, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMSXMUijGN0t"
   },
   "source": [
    "#### Feature Importance\n",
    "트리 기반 모델은 트리를 분기하는 과정에서 어떤 변수가 모델을 생성하는데 중요한지에 대한 변수 중요도를 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63Dh2Pd2GN0t"
   },
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(xgb.feature_importances_.reshape((1, -1)), columns=x_train.columns, index=['feature_importance'])\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXOgVrKHGN0t"
   },
   "source": [
    "### LightGBM\n",
    "XGBoost가 기존의 부스팅 트리 모델보다는 학습이 빠르다는 장점이 있지만, 여전히 속도가 느린 알고리즘입니다. <br>\n",
    "LightGBM은 XGBoost보다 가볍고 더 나은 학습 성능을 제공하며, 더 적은 메모리를 사용합니다. <br>\n",
    "다른 부스팅 트리와는 다른 특징으로 `균형 트리 분할 방식`으로 모델을 학습시키는 것이 아닌 `리프 중심 트리 분할`방식을 사용해 비대칭적인 트리를 형성하게 됩니다. <br>\n",
    "예측 성능 자체는 XGBoost와 비슷하지만 학습 속도 및 메모리 사용량에서 이점을 갖습니다. \n",
    "\n",
    "1. XGBoost의 장점 + \n",
    "2. XGBoost 보다 가볍고 빠른 모델, 하지만 더 나은 학습 성능\n",
    "3. Leaf-wise tree growth (대체적으로 더 나은 성능을 보장하지만, 적은 데이터에서 과적합 우려)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsZ6QsmnGN0t"
   },
   "source": [
    "- LightGBM 대표적 파라미터\n",
    "    - n_estimators      (int)  : 내부에서 생성할 결정 트리의 개수 \n",
    "    - max_depth         (int)  : 생성할 결정 트리의 높이\n",
    "    - learning_rate     (float): 훈련량, 학습 시 모델을 얼마나 업데이트할지 결정하는 값\n",
    "    - colsample_bytree  (float): 열 샘플링에 사용하는 비율\n",
    "    - subsample         (float): 행 샘플링에 사용하는 비율\n",
    "    - reg_alpha         (float): L1 정규화 계수\n",
    "    - reg_lambda        (float): L2 정규화 계수\n",
    "    - boosting_type     (str)  : 부스팅 방법 (gbdt / rf / dart / goss)\n",
    "    - random_state      (int)  : 내부적으로 사용되는 난수값\n",
    "    - n_jobs            (int)  : 병렬처리에 사용할 CPU 수\n",
    "    \n",
    "#### ref\n",
    "- [LightGBM greatjoy님 블로그](https://greatjoy.tistory.com/72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng2mLTzXGN0u"
   },
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gt4HoZ5sGN0u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4vvjLQCGN0u"
   },
   "source": [
    "#### 2) 모델 학습하기 (훈련 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKYME7-dGN0u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-eHIPvmGN0u"
   },
   "source": [
    "#### 3) 결과 예측하기 (테스트 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZY-HV_uGN0v"
   },
   "outputs": [],
   "source": [
    "y_pred = \n",
    "y_pred_proba = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzTTtJJVGN0v"
   },
   "source": [
    "#### 4) 결과 살펴보기\n",
    "일반적으로 분류에서는 Accuracy, 정확도를 평가 척도로 사용합니다. + Log loss<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jukuTXHvGN0v"
   },
   "outputs": [],
   "source": [
    "print('LightGBM, 정확도 : {:.4f}'.format(accuracy_score(y_valid, y_pred)))\n",
    "print('LightGBM, Log loss : {:.4f}'.format(log_loss(y_valid, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6gykVhTGN0v"
   },
   "source": [
    "#### Feature Importance\n",
    "트리 기반 모델은 트리를 분기하는 과정에서 어떤 변수가 모델을 생성하는데 중요한지에 대한 변수 중요도를 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyDc2yc3GN0w"
   },
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(lgb.feature_importances_.reshape((1, -1)), columns=x_train.columns, index=['feature_importance'])\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6Ba0JomGN0w"
   },
   "source": [
    "## Evaluation \n",
    "### 1. Accuracy, 정확도\n",
    "모든 데이터에 대해 클래스 라벨을 얼마나 잘 맞췄는지를 계산 \n",
    "\n",
    "<img src= './img/Accuracy.png' alter='Accuracy' style=\"height: 350px\"/> \n",
    "\n",
    "### 2. Confusion Matrix, 혼동 행렬\n",
    "정확도로는 분류 모델의 평가가 충분하지 않을 수 있습니다. 예를 들어, 병이 있는 사람을 병이 없다고 판단하는 경우 Risk가 높기 때문에 모델의 목적에 맞게 분류 모델을 평가하여야 합니다. 이때 사용되는 것이 Confusion Matrix 입니다. \n",
    "\n",
    "<img src= './img/Confusion_Matrix.png' alter='Confusion_Matrix' style=\"height: 200px\"/> \n",
    "\n",
    "* Precision, 정밀도  : TP/(FP+TP), 1이라고 예측한 것 중 실제로 1인 것 \n",
    "    - 모델이 1이라고 예측한 것 중에 실제로 1인 것\n",
    "* Recall, 재현율 : True Positive rate = Recall = Hit ratio = TP/(TP+FN), 실제로 1인 것 중에 1이라고 예측한 것 \n",
    "    - 실제로 1인 것 중에 모델이 1이라고 예측한 것, 질병 예측에서 가장 중요한 척도 중 하나, 예측을 잘못하면 사람이 죽을 수도 있기 때문에\n",
    "* False Alarm, 오탐 : False Positive rate = 1-Specificity = FP/(FP+TN), 실제로 0인 것 중에 1이라고 예측한 것\n",
    "    - 이게 코로나 검사에서 안걸렸는데 걸렸다고 하는 경우 입니다.\n",
    "* F1 Score, 정밀도와 재현율의 조화 평균 : 클래스 불균형한 문제에서 주로 사용되는 평가지표\n",
    "\n",
    "### 3. ROC Curve, AUC\n",
    "ROC Curve(Receiver-Operating Characteristic curve)는 민감도와 특이도가 서로 어떤 관계를 가지며 변하는지를 2차원 평면상에 표현한 것 입니다.<br>\n",
    "ROC Curve가 그려지는 곡선을 의미하고, AUC(Area Under Curve)는 ROC Curve의 면적을 뜻합니다.<br>\n",
    "AUC 값이 1에 가까울 수록 좋은 모델을 의미합니다. \n",
    "\n",
    "<img src= './img/ROC_AUC.png' alter='ROC_AUC' style=\"height: 500px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLR0iFLSGN0w"
   },
   "source": [
    "#### 정확도, 정밀도, 민감도, AUC Score\n",
    "- 클래스가 불균형한 데이터인 경우 정확도로 모델의 성능을 판단하는건 적절하지 않습니다. \n",
    "    - 이러한 경우에는 정밀도나 재현율 또는 정밀도와 재현율의 조화 평균인 F1 Score를 사용하기도 합니다.\n",
    "    \n",
    "#### ref\n",
    "- [ROC AUC adnoctum님 블로그](https://adnoctum.tistory.com/121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cS_s27UkGN0w"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBZNAX0BGN0w"
   },
   "outputs": [],
   "source": [
    "print('Accuracy : {:.3f}'.format(accuracy_score(y_valid, y_pred)))\n",
    "print('Precision : {:.3f}'.format(precision_score(y_valid, y_pred)))\n",
    "print('Recall : {:.3f}'.format(recall_score(y_valid, y_pred)))\n",
    "print('F1 Score: {:.3f}'.format(f1_score(y_valid, y_pred)))\n",
    "print('AUC : {:.3f}'.format(roc_auc_score(y_valid, y_pred_proba[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8YA17OQGN0x"
   },
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxsx1MnIGN0x"
   },
   "source": [
    "## 실습 솔루션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQCJCtaFGN0x"
   },
   "source": [
    "### 1) 전처리 실습 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuA1VWUvGN0x"
   },
   "source": [
    "#### 1. 결측치 처리\n",
    "`occyp_type` 변수 최빈 값으로 결측치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4CNub49GN0x"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "x_train[cat_columns] = imputer.fit_transform(x_train[cat_columns])\n",
    "x_valid[cat_columns] = imputer.transform(x_valid[cat_columns])\n",
    "x_test[cat_columns]  = imputer.transform(x_test[cat_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkVqTLqQGN0x"
   },
   "source": [
    "#### 2. 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohy4xp0jGN0y"
   },
   "outputs": [],
   "source": [
    "x_train_mean = np.mean(x_train[num_columns], axis=0)\n",
    "x_train_std  = np.std(x_train[num_columns], axis=0)\n",
    "\n",
    "# Numpy 브로드캐스팅 기능을 활용해 x_train의 평균과 표준편차로 x_train, x_valid의 스케일링을 진행해줍니다.\n",
    "x_train.loc[:, num_columns] = (x_train[num_columns] - x_train_mean) / (x_train_std + 1e-10)\n",
    "x_valid.loc[:, num_columns] = (x_valid[num_columns] - x_train_mean) / (x_train_std + 1e-10)\n",
    "x_test.loc[:, num_columns]  = (x_test[num_columns]  - x_train_mean) / (x_train_std + 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j33EmDwpGN0y"
   },
   "source": [
    "#### 스케일링 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dfCseMZGN0y"
   },
   "outputs": [],
   "source": [
    "x_train[num_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wm3DuXZ-GN0y"
   },
   "outputs": [],
   "source": [
    "x_valid[num_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_anvWQEGN0y"
   },
   "outputs": [],
   "source": [
    "x_test[num_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLcMWRk-GN0z"
   },
   "source": [
    "#### 왜 x_valid와 x_test는 평균이 0, 표준편차가 1이 아닌가요? -> x_train의 평균과 표준편차를 사용했기 때문에 x_valid의 평균과 표준편차가 0, 1이 아닐 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fo11zWyAGN0z"
   },
   "source": [
    "#### 3. 범주형 변수 OneHot Encoding, 라벨 변수 Label Encoding \n",
    "- 범주형 변수 OneHot Encoding 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrVMjTNoGN0z"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGdFB8dHGN0z"
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "ohe.fit(x_train[cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d9jRCKsGN0z"
   },
   "outputs": [],
   "source": [
    "# new_x_train_cat = ohe.transform(x_train[cat_columns])\n",
    "# new_x_valid_cat = ohe.transform(x_valid[cat_columns])\n",
    "# new_x_test_cat = ohe.transform(x_test[cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3bOOnEmGN0z"
   },
   "outputs": [],
   "source": [
    "# 둘다 가능\n",
    "x_all = pd.concat([x_train[cat_columns], x_valid[cat_columns], x_test[cat_columns]], axis=0)\n",
    "\n",
    "# 전체에 대해 fiting\n",
    "ohe.fit(x_all)\n",
    "\n",
    "# ohe.categories_ 은 입력된 범주형 컬럼의 범주 값을 순서대로 담고 있습니다.\n",
    "ohe_columns = ohe.categories_[0].tolist()\n",
    "for col in ohe.categories_[1:]:\n",
    "    ohe_columns += col.tolist()\n",
    "ohe_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OH-QGoxIGN00"
   },
   "outputs": [],
   "source": [
    "new_x_train_cat = pd.DataFrame(ohe.transform(x_train[cat_columns]), columns=ohe_columns)\n",
    "new_x_valid_cat = pd.DataFrame(ohe.transform(x_valid[cat_columns]), columns=ohe_columns)\n",
    "new_x_test_cat  = pd.DataFrame(ohe.transform(x_test[cat_columns]),  columns=ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fopnxEO2GN00"
   },
   "outputs": [],
   "source": [
    "# train set 개수 확인\n",
    "new_x_train_cat.shape, x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3VSDgt0GN00"
   },
   "outputs": [],
   "source": [
    "# valid set 개수 확인\n",
    "new_x_valid_cat.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yLrwXHHGN00"
   },
   "outputs": [],
   "source": [
    "# test set 개수 확인\n",
    "new_x_test_cat.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2VvUGauGN01"
   },
   "outputs": [],
   "source": [
    "# 동일하게 데이터를 쪼갤 시 인덱스를 초기화합니다.\n",
    "new_x_train_cat.reset_index(drop=True, inplace=True)\n",
    "new_x_valid_cat.reset_index(drop=True, inplace=True)\n",
    "new_x_test_cat.reset_index(drop=True,  inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itnYcUlZGN01"
   },
   "source": [
    "##### 기존 범주형 변수를 제거하고, Onehot Encoding된 변수를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaeP-_kyGN01"
   },
   "outputs": [],
   "source": [
    "# Onehot Encoding 변수 추가\n",
    "x_train = pd.concat([x_train[num_columns], new_x_train_cat], axis=1)\n",
    "x_valid = pd.concat([x_valid[num_columns], new_x_valid_cat], axis=1)\n",
    "x_test  = pd.concat([x_test[num_columns],  new_x_test_cat],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzY1FyK_GN01"
   },
   "outputs": [],
   "source": [
    "# train 확인\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Iu439GwGN02"
   },
   "outputs": [],
   "source": [
    "# valid 확인\n",
    "x_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-GL3d06GN02"
   },
   "outputs": [],
   "source": [
    "# test 확인\n",
    "x_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_BJiJEZGN02"
   },
   "source": [
    "라벨 변수 인코딩 (범주형 변수인 경우)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KRS25KFGN02"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNrp0iPwGN02"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFPA4FIKGN02"
   },
   "outputs": [],
   "source": [
    "# 인코딩 전\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nX1NXa9VGN03"
   },
   "outputs": [],
   "source": [
    "y_train = le.transform(y_train)\n",
    "y_valid = le.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g39nuSoRGN03"
   },
   "outputs": [],
   "source": [
    "# 인코딩 후\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSMIKS50GN03"
   },
   "source": [
    "    이미 수치형 데이터이므로\n",
    "    다음과 같이 처리해도 무방합니다.\n",
    "    y_train = y_train.astype(int)\n",
    "    y_valid = y_valid.astype(int)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_02) Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Kyle",
   "language": "python",
   "name": "kyle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
